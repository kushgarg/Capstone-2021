{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0004de6046f1b3d314f33fdb43a2dc798b2646e5600efd8df5066c8b63a00ff6d",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, add\n",
    "from keras.layers.core import  Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "bytes_before_compression= []\n",
    "bytes_after_compression = [] \n",
    "compression_ratio = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_01.nc']\n",
    "# file_path.append('C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_02.nc')\n",
    "# file_path.append('C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_03.nc')\n",
    "# file_path.append('C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_04.nc')\n",
    "# file_path.append('C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_05.nc')\n",
    "# file_path.append('C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_06.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1593 - val_loss: 0.1229\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1238 - val_loss: 0.1087\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1096 - val_loss: 0.0980\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0988 - val_loss: 0.0820\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0827 - val_loss: 0.0630\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0636 - val_loss: 0.0461\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0465 - val_loss: 0.0338\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0341 - val_loss: 0.0259\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0261 - val_loss: 0.0209\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0210 - val_loss: 0.0174\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0174 - val_loss: 0.0145\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0145 - val_loss: 0.0120\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0099 - val_loss: 0.0083\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0083 - val_loss: 0.0070\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0071 - val_loss: 0.0061\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0010 - val_loss: 9.4457e-04\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.6497e-04 - val_loss: 8.7070e-04\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.9419e-04 - val_loss: 8.0022e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.2648e-04 - val_loss: 7.3571e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6423e-04 - val_loss: 6.8106e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.1132e-04 - val_loss: 6.3865e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.7032e-04 - val_loss: 6.0657e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.3938e-04 - val_loss: 5.7879e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.1239e-04 - val_loss: 5.4939e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.8321e-04 - val_loss: 5.1665e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.5003e-04 - val_loss: 4.8315e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.1548e-04 - val_loss: 4.5294e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.8377e-04 - val_loss: 4.2859e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.5775e-04 - val_loss: 4.0954e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.3708e-04 - val_loss: 3.9278e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.1898e-04 - val_loss: 3.7596e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0128e-04 - val_loss: 3.5897e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8398e-04 - val_loss: 3.4277e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6803e-04 - val_loss: 3.2796e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.5394e-04 - val_loss: 3.1443e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4146e-04 - val_loss: 3.0142e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.2956e-04 - val_loss: 2.8852e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1751e-04 - val_loss: 2.7662e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0601e-04 - val_loss: 2.6692e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9622e-04 - val_loss: 2.5947e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8829e-04 - val_loss: 2.5337e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8154e-04 - val_loss: 2.4781e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7540e-04 - val_loss: 2.4215e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6937e-04 - val_loss: 2.3615e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6328e-04 - val_loss: 2.3028e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5748e-04 - val_loss: 2.2500e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5233e-04 - val_loss: 2.2019e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.4760e-04 - val_loss: 2.1554e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4297e-04 - val_loss: 2.1111e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.3852e-04 - val_loss: 2.0733e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.3465e-04 - val_loss: 2.0443e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.3159e-04 - val_loss: 2.0231e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.2919e-04 - val_loss: 2.0051e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.2700e-04 - val_loss: 1.9866e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.2466e-04 - val_loss: 1.9673e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.2224e-04 - val_loss: 1.9484e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1993e-04 - val_loss: 1.9295e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1776e-04 - val_loss: 1.9099e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.1563e-04 - val_loss: 1.8895e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.1355e-04 - val_loss: 1.8694e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.1158e-04 - val_loss: 1.8511e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.0981e-04 - val_loss: 1.8349e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0828e-04 - val_loss: 1.8199e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0686e-04 - val_loss: 1.8056e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.0550e-04 - val_loss: 1.7928e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.0423e-04 - val_loss: 1.7817e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0308e-04 - val_loss: 1.7719e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.0199e-04 - val_loss: 1.7628e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.0088e-04 - val_loss: 1.7537e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.9973e-04 - val_loss: 1.7446e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.9858e-04 - val_loss: 1.7356e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9746e-04 - val_loss: 1.7271e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9639e-04 - val_loss: 1.7193e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.9540e-04 - val_loss: 1.7123e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.9450e-04 - val_loss: 1.7060e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.9366e-04 - val_loss: 1.7000e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.9287e-04 - val_loss: 1.6941e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9211e-04 - val_loss: 1.6883e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.9137e-04 - val_loss: 1.6826e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9065e-04 - val_loss: 1.6770e-04\n",
      "(11, 150)\n",
      "(11, 1500)\n",
      "66112\n"
     ]
    }
   ],
   "source": [
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 30\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "import sys\n",
    "# encoder\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_01.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[9.8351]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  dataset  bytes_before_compression  bytes_after_compression  \\\n",
       "0  ocean_eta_t_2000_01.nc                     66112                     6712   \n",
       "1  ocean_eta_t_2000_02.nc                     60112                     6112   \n",
       "2  ocean_eta_t_2000_03.nc                     66112                     6712   \n",
       "3  ocean_eta_t_2000_04.nc                     60112                     6112   \n",
       "4  ocean_eta_t_2000_05.nc                     66112                     6712   \n",
       "5  ocean_eta_t_2000_06.nc                     60112                     6112   \n",
       "\n",
       "   compression_ratio  \n",
       "0             9.8498  \n",
       "1             9.8351  \n",
       "2             9.8498  \n",
       "3             9.8351  \n",
       "4             9.8498  \n",
       "5             9.8351  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>bytes_before_compression</th>\n      <th>bytes_after_compression</th>\n      <th>compression_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ocean_eta_t_2000_01.nc</td>\n      <td>66112</td>\n      <td>6712</td>\n      <td>9.8498</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ocean_eta_t_2000_02.nc</td>\n      <td>60112</td>\n      <td>6112</td>\n      <td>9.8351</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ocean_eta_t_2000_03.nc</td>\n      <td>66112</td>\n      <td>6712</td>\n      <td>9.8498</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ocean_eta_t_2000_04.nc</td>\n      <td>60112</td>\n      <td>6112</td>\n      <td>9.8351</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ocean_eta_t_2000_05.nc</td>\n      <td>66112</td>\n      <td>6712</td>\n      <td>9.8498</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ocean_eta_t_2000_06.nc</td>\n      <td>60112</td>\n      <td>6112</td>\n      <td>9.8351</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"dataset\":dataset, \"bytes_before_compression\":bytes_before_compression,'bytes_after_compression':bytes_after_compression,'compression_ratio':compression_ratio}).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "source": [
    "## er"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(29, 1500)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "time_ytocean_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 0.1509 - val_loss: 0.1205\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1208 - val_loss: 0.1049\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1052 - val_loss: 0.0887\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0888 - val_loss: 0.0696\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0697 - val_loss: 0.0515\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0515 - val_loss: 0.0376\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0375 - val_loss: 0.0279\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0278 - val_loss: 0.0216\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0215 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0173 - val_loss: 0.0144\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0143 - val_loss: 0.0120\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0099 - val_loss: 0.0084\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0072 - val_loss: 0.0064\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0064 - val_loss: 0.0057\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0011 - val_loss: 9.9505e-04\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0010 - val_loss: 9.3718e-04\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.4339e-04 - val_loss: 8.8279e-04\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.8924e-04 - val_loss: 8.2794e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.3410e-04 - val_loss: 7.6859e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7403e-04 - val_loss: 7.0581e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.1023e-04 - val_loss: 6.4595e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.4915e-04 - val_loss: 5.9526e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.9708e-04 - val_loss: 5.5560e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 5.5599e-04 - val_loss: 5.2501e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 5.2407e-04 - val_loss: 5.0025e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.9825e-04 - val_loss: 4.7854e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.7590e-04 - val_loss: 4.5790e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.5503e-04 - val_loss: 4.3661e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.3375e-04 - val_loss: 4.1409e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.1131e-04 - val_loss: 3.9199e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.8927e-04 - val_loss: 3.7220e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.6948e-04 - val_loss: 3.5493e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 3.5217e-04 - val_loss: 3.3964e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.3678e-04 - val_loss: 3.2597e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.2304e-04 - val_loss: 3.1341e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1057e-04 - val_loss: 3.0146e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9894e-04 - val_loss: 2.9028e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.8835e-04 - val_loss: 2.8037e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7914e-04 - val_loss: 2.7172e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7106e-04 - val_loss: 2.6399e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6354e-04 - val_loss: 2.5705e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5637e-04 - val_loss: 2.5089e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.4966e-04 - val_loss: 2.4533e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.4340e-04 - val_loss: 2.4040e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.3780e-04 - val_loss: 2.3641e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.3320e-04 - val_loss: 2.3321e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.2946e-04 - val_loss: 2.3018e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.2594e-04 - val_loss: 2.2691e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.2229e-04 - val_loss: 2.2329e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1852e-04 - val_loss: 2.1937e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.1474e-04 - val_loss: 2.1536e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.1112e-04 - val_loss: 2.1154e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.0783e-04 - val_loss: 2.0822e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0498e-04 - val_loss: 2.0541e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.0254e-04 - val_loss: 2.0298e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0033e-04 - val_loss: 2.0079e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9823e-04 - val_loss: 1.9871e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.9612e-04 - val_loss: 1.9671e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9403e-04 - val_loss: 1.9494e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9210e-04 - val_loss: 1.9338e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9034e-04 - val_loss: 1.9189e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.8862e-04 - val_loss: 1.9044e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.8693e-04 - val_loss: 1.8914e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8537e-04 - val_loss: 1.8803e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8405e-04 - val_loss: 1.8704e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8293e-04 - val_loss: 1.8602e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.8189e-04 - val_loss: 1.8493e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.8087e-04 - val_loss: 1.8376e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.7984e-04 - val_loss: 1.8262e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7885e-04 - val_loss: 1.8157e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7794e-04 - val_loss: 1.8064e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7708e-04 - val_loss: 1.7978e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7622e-04 - val_loss: 1.7900e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7535e-04 - val_loss: 1.7828e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7451e-04 - val_loss: 1.7761e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.7374e-04 - val_loss: 1.7696e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7302e-04 - val_loss: 1.7629e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7234e-04 - val_loss: 1.7561e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.7166e-04 - val_loss: 1.7497e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7099e-04 - val_loss: 1.7441e-04\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002D5A7CCA940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(10, 150)\n",
      "(10, 1500)\n",
      "60112\n"
     ]
    }
   ],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_02.nc']\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 30\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_02.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  },
  {
   "source": [
    "## san\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 0.1650 - val_loss: 0.1318\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1317 - val_loss: 0.1165\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1165 - val_loss: 0.1032\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1032 - val_loss: 0.0851\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0851 - val_loss: 0.0650\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0651 - val_loss: 0.0476\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0478 - val_loss: 0.0348\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0351 - val_loss: 0.0263\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0266 - val_loss: 0.0207\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0211 - val_loss: 0.0169\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0173 - val_loss: 0.0139\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0143 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0118 - val_loss: 0.0096\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0098 - val_loss: 0.0082\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0066 - val_loss: 0.0060\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0010 - val_loss: 9.7675e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.5600e-04 - val_loss: 9.2826e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.0357e-04 - val_loss: 8.7290e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4599e-04 - val_loss: 8.1395e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.8618e-04 - val_loss: 7.5663e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.2892e-04 - val_loss: 7.0408e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.7710e-04 - val_loss: 6.5650e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.3088e-04 - val_loss: 6.1363e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.9003e-04 - val_loss: 5.7518e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.5420e-04 - val_loss: 5.4087e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.2295e-04 - val_loss: 5.1200e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.9721e-04 - val_loss: 4.8995e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.7790e-04 - val_loss: 4.7353e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.6352e-04 - val_loss: 4.5927e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5044e-04 - val_loss: 4.4320e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.3463e-04 - val_loss: 4.2329e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.1415e-04 - val_loss: 4.0144e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.9107e-04 - val_loss: 3.8170e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.6964e-04 - val_loss: 3.6674e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5282e-04 - val_loss: 3.5658e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.4091e-04 - val_loss: 3.4937e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.3237e-04 - val_loss: 3.4276e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2505e-04 - val_loss: 3.3499e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.1731e-04 - val_loss: 3.2533e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0833e-04 - val_loss: 3.1428e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9842e-04 - val_loss: 3.0330e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8870e-04 - val_loss: 2.9363e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8014e-04 - val_loss: 2.8581e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7307e-04 - val_loss: 2.7990e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6753e-04 - val_loss: 2.7548e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6320e-04 - val_loss: 2.7183e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5946e-04 - val_loss: 2.6833e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5575e-04 - val_loss: 2.6467e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5177e-04 - val_loss: 2.6081e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.4749e-04 - val_loss: 2.5692e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.4312e-04 - val_loss: 2.5323e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.3894e-04 - val_loss: 2.4996e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.3526e-04 - val_loss: 2.4716e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.3217e-04 - val_loss: 2.4465e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2949e-04 - val_loss: 2.4219e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2694e-04 - val_loss: 2.3966e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.2439e-04 - val_loss: 2.3714e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.2192e-04 - val_loss: 2.3474e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1957e-04 - val_loss: 2.3242e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.1736e-04 - val_loss: 2.3024e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1531e-04 - val_loss: 2.2821e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.1345e-04 - val_loss: 2.2631e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.1176e-04 - val_loss: 2.2448e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.1016e-04 - val_loss: 2.2271e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.0857e-04 - val_loss: 2.2105e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.0698e-04 - val_loss: 2.1960e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.0544e-04 - val_loss: 2.1838e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.0400e-04 - val_loss: 2.1736e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0265e-04 - val_loss: 2.1646e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.0135e-04 - val_loss: 2.1568e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0010e-04 - val_loss: 2.1501e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.9895e-04 - val_loss: 2.1441e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.9791e-04 - val_loss: 2.1378e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.9699e-04 - val_loss: 2.1303e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9611e-04 - val_loss: 2.1212e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.9523e-04 - val_loss: 2.1112e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.9435e-04 - val_loss: 2.1011e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.9351e-04 - val_loss: 2.0913e-04\n",
      "(11, 150)\n",
      "(11, 1500)\n",
      "66112\n"
     ]
    }
   ],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_03.nc']\n",
    "\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 100\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_03.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  },
  {
   "source": [
    "## si"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.1681 - val_loss: 0.1300\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1302 - val_loss: 0.1105\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1106 - val_loss: 0.0952\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0953 - val_loss: 0.0775\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0777 - val_loss: 0.0586\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0589 - val_loss: 0.0427\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0431 - val_loss: 0.0316\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0320 - val_loss: 0.0244\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0249 - val_loss: 0.0197\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0201 - val_loss: 0.0162\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0166 - val_loss: 0.0135\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0137 - val_loss: 0.0113\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0097 - val_loss: 0.0083\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0083 - val_loss: 0.0073\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0065 - val_loss: 0.0059\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.4060e-04 - val_loss: 0.0010\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.7514e-04 - val_loss: 9.4385e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2248e-04 - val_loss: 8.9292e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.7884e-04 - val_loss: 8.4618e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.3968e-04 - val_loss: 8.0085e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.0136e-04 - val_loss: 7.5565e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.6183e-04 - val_loss: 7.1178e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.2169e-04 - val_loss: 6.7205e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.8340e-04 - val_loss: 6.3788e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.4846e-04 - val_loss: 6.0871e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.1683e-04 - val_loss: 5.8361e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.8845e-04 - val_loss: 5.6216e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.6379e-04 - val_loss: 5.4379e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.4294e-04 - val_loss: 5.2746e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.2516e-04 - val_loss: 5.1199e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.0936e-04 - val_loss: 4.9641e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.9448e-04 - val_loss: 4.8046e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8014e-04 - val_loss: 4.6482e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.6682e-04 - val_loss: 4.5019e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5487e-04 - val_loss: 4.3649e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.4380e-04 - val_loss: 4.2367e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.3314e-04 - val_loss: 4.1222e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.2311e-04 - val_loss: 4.0254e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 3.1407e-04 - val_loss: 3.9456e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 3.0607e-04 - val_loss: 3.8798e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9902e-04 - val_loss: 3.8247e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9277e-04 - val_loss: 3.7760e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8697e-04 - val_loss: 3.7290e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8123e-04 - val_loss: 3.6827e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7551e-04 - val_loss: 3.6402e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7032e-04 - val_loss: 3.6045e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6616e-04 - val_loss: 3.5721e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6288e-04 - val_loss: 3.5352e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5971e-04 - val_loss: 3.4898e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5606e-04 - val_loss: 3.4406e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5217e-04 - val_loss: 3.3959e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.4863e-04 - val_loss: 3.3602e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.4576e-04 - val_loss: 3.3327e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.4339e-04 - val_loss: 3.3102e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.4116e-04 - val_loss: 3.2905e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.3893e-04 - val_loss: 3.2731e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.3674e-04 - val_loss: 3.2573e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.3466e-04 - val_loss: 3.2415e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.3266e-04 - val_loss: 3.2247e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.3069e-04 - val_loss: 3.2066e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.2876e-04 - val_loss: 3.1882e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.2696e-04 - val_loss: 3.1709e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.2538e-04 - val_loss: 3.1550e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.2400e-04 - val_loss: 3.1401e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2274e-04 - val_loss: 3.1255e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.2150e-04 - val_loss: 3.1117e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.2027e-04 - val_loss: 3.0997e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.1908e-04 - val_loss: 3.0894e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.1792e-04 - val_loss: 3.0803e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.1676e-04 - val_loss: 3.0722e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.1565e-04 - val_loss: 3.0647e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.1464e-04 - val_loss: 3.0574e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.1371e-04 - val_loss: 3.0495e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.1284e-04 - val_loss: 3.0403e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.1195e-04 - val_loss: 3.0298e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.1106e-04 - val_loss: 3.0186e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.1017e-04 - val_loss: 3.0076e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0933e-04 - val_loss: 2.9975e-04\n",
      "(10, 150)\n",
      "(10, 1500)\n",
      "60112\n"
     ]
    }
   ],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_04.nc']\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 100\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_04.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.1656 - val_loss: 0.1317\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1309 - val_loss: 0.1136\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1129 - val_loss: 0.0997\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0992 - val_loss: 0.0835\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0831 - val_loss: 0.0654\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0650 - val_loss: 0.0489\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0485 - val_loss: 0.0364\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0360 - val_loss: 0.0281\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0277 - val_loss: 0.0227\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0224 - val_loss: 0.0188\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0185 - val_loss: 0.0156\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0154 - val_loss: 0.0130\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0128 - val_loss: 0.0109\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0091 - val_loss: 0.0080\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0078 - val_loss: 0.0069\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0067 - val_loss: 0.0060\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 9.8450e-04 - val_loss: 9.5702e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 9.2339e-04 - val_loss: 9.0073e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.6803e-04 - val_loss: 8.4455e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.1371e-04 - val_loss: 7.8804e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.5951e-04 - val_loss: 7.3372e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.0744e-04 - val_loss: 6.8498e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.6050e-04 - val_loss: 6.4436e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.2112e-04 - val_loss: 6.1184e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.8936e-04 - val_loss: 5.8534e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.6343e-04 - val_loss: 5.6215e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.4089e-04 - val_loss: 5.3988e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.1950e-04 - val_loss: 5.1734e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.9793e-04 - val_loss: 4.9443e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.7577e-04 - val_loss: 4.7212e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.5374e-04 - val_loss: 4.5199e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.3344e-04 - val_loss: 4.3506e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.1616e-04 - val_loss: 4.2109e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.0203e-04 - val_loss: 4.0883e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.9000e-04 - val_loss: 3.9720e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.7896e-04 - val_loss: 3.8561e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6808e-04 - val_loss: 3.7376e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.5684e-04 - val_loss: 3.6200e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.4558e-04 - val_loss: 3.5090e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.3500e-04 - val_loss: 3.4069e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2542e-04 - val_loss: 3.3165e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1708e-04 - val_loss: 3.2412e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1011e-04 - val_loss: 3.1787e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0411e-04 - val_loss: 3.1200e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9813e-04 - val_loss: 3.0590e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9167e-04 - val_loss: 2.9974e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8506e-04 - val_loss: 2.9397e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7890e-04 - val_loss: 2.8886e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7361e-04 - val_loss: 2.8450e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6931e-04 - val_loss: 2.8065e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6576e-04 - val_loss: 2.7684e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6244e-04 - val_loss: 2.7278e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5898e-04 - val_loss: 2.6853e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5537e-04 - val_loss: 2.6433e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5182e-04 - val_loss: 2.6045e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.4851e-04 - val_loss: 2.5705e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.4553e-04 - val_loss: 2.5421e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.4289e-04 - val_loss: 2.5185e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4053e-04 - val_loss: 2.4972e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.3831e-04 - val_loss: 2.4755e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.3610e-04 - val_loss: 2.4528e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.3388e-04 - val_loss: 2.4299e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.3174e-04 - val_loss: 2.4083e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.2975e-04 - val_loss: 2.3882e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2791e-04 - val_loss: 2.3695e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.2624e-04 - val_loss: 2.3510e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.2466e-04 - val_loss: 2.3318e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2310e-04 - val_loss: 2.3123e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.2153e-04 - val_loss: 2.2940e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.2002e-04 - val_loss: 2.2782e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.1859e-04 - val_loss: 2.2649e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.1726e-04 - val_loss: 2.2534e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.1599e-04 - val_loss: 2.2429e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.1476e-04 - val_loss: 2.2326e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.1354e-04 - val_loss: 2.2219e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.1236e-04 - val_loss: 2.2104e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.1124e-04 - val_loss: 2.1980e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.1016e-04 - val_loss: 2.1850e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0914e-04 - val_loss: 2.1719e-04\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002D597D07040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(11, 150)\n",
      "(11, 1500)\n",
      "66112\n"
     ]
    }
   ],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_05.nc']\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 100\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_05.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1500)]            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 150)               225150    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1500)              226500    \n",
      "=================================================================\n",
      "Total params: 451,650\n",
      "Trainable params: 451,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.1580 - val_loss: 0.1286\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1283 - val_loss: 0.1141\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1138 - val_loss: 0.1005\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1002 - val_loss: 0.0818\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0815 - val_loss: 0.0620\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0618 - val_loss: 0.0459\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0456 - val_loss: 0.0344\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0341 - val_loss: 0.0264\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0261 - val_loss: 0.0209\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0206 - val_loss: 0.0170\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0167 - val_loss: 0.0141\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0139 - val_loss: 0.0120\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0102 - val_loss: 0.0089\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0088 - val_loss: 0.0076\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0041\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0010 - val_loss: 9.7713e-04\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.2614e-04 - val_loss: 9.0736e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.5375e-04 - val_loss: 8.4846e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.9227e-04 - val_loss: 7.9848e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.4018e-04 - val_loss: 7.5432e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.9454e-04 - val_loss: 7.1436e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.5353e-04 - val_loss: 6.7721e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 6.1541e-04 - val_loss: 6.4030e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.7745e-04 - val_loss: 6.0269e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.3882e-04 - val_loss: 5.6721e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.0240e-04 - val_loss: 5.3713e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.7150e-04 - val_loss: 5.1178e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.4559e-04 - val_loss: 4.8818e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.2173e-04 - val_loss: 4.6538e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.9888e-04 - val_loss: 4.4482e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.7843e-04 - val_loss: 4.2770e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.6143e-04 - val_loss: 4.1350e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.4716e-04 - val_loss: 4.0040e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3379e-04 - val_loss: 3.8697e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.1992e-04 - val_loss: 3.7334e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0565e-04 - val_loss: 3.6031e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9191e-04 - val_loss: 3.4846e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7935e-04 - val_loss: 3.3793e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6810e-04 - val_loss: 3.2864e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5800e-04 - val_loss: 3.2045e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.4890e-04 - val_loss: 3.1333e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.4080e-04 - val_loss: 3.0703e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.3367e-04 - val_loss: 3.0126e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.2742e-04 - val_loss: 2.9586e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.2192e-04 - val_loss: 2.9059e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.1693e-04 - val_loss: 2.8526e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.1219e-04 - val_loss: 2.8005e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.0769e-04 - val_loss: 2.7547e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.0372e-04 - val_loss: 2.7183e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.0039e-04 - val_loss: 2.6893e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.9744e-04 - val_loss: 2.6628e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.9448e-04 - val_loss: 2.6378e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.9148e-04 - val_loss: 2.6156e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.8869e-04 - val_loss: 2.5956e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.8618e-04 - val_loss: 2.5748e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.8382e-04 - val_loss: 2.5520e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.8150e-04 - val_loss: 2.5271e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.7932e-04 - val_loss: 2.5029e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7735e-04 - val_loss: 2.4796e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.7559e-04 - val_loss: 2.4587e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.7398e-04 - val_loss: 2.4406e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.7242e-04 - val_loss: 2.4247e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.7089e-04 - val_loss: 2.4113e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6941e-04 - val_loss: 2.3991e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6802e-04 - val_loss: 2.3877e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.6672e-04 - val_loss: 2.3769e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6550e-04 - val_loss: 2.3664e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.6437e-04 - val_loss: 2.3572e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.6331e-04 - val_loss: 2.3485e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.6228e-04 - val_loss: 2.3397e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6127e-04 - val_loss: 2.3305e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.6032e-04 - val_loss: 2.3209e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.5946e-04 - val_loss: 2.3109e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.5868e-04 - val_loss: 2.3006e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5790e-04 - val_loss: 2.2904e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5714e-04 - val_loss: 2.2807e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5642e-04 - val_loss: 2.2724e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5575e-04 - val_loss: 2.2646e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.5510e-04 - val_loss: 2.2578e-04\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002D599076670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(10, 150)\n",
      "(10, 1500)\n",
      "60112\n"
     ]
    }
   ],
   "source": [
    "file_path =['C:/Users/Administrator/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_06.nc']\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t_arr = ds.variables['eta_t'][:]\n",
    "time_ytocean_arr = eta_t_arr[:,:,0]\n",
    "time_ytocean_arr.shape\n",
    "time_ytocean_arr = np.divide(time_ytocean_arr, 2)\n",
    "time_ytocean_arr[0]\n",
    "for i in range(0,len(time_ytocean_arr)):\n",
    "    arr = time_ytocean_arr[i].data\n",
    "    arr[arr == -16384] = 0\n",
    "time_ytocean_train, time_ytocean_test = train_test_split(time_ytocean_arr.data, test_size=0.33)\n",
    "time_ytocean_train.shape\n",
    "input_size = 1500\n",
    "hidden_size = 150\n",
    "output_size = 1500\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "h = Dense(hidden_size, activation='tanh')(x)\n",
    "r = Dense(output_size, activation='tanh')(h)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "epochs = 100\n",
    "batch_size = 1500\n",
    "\n",
    "history = autoencoder.fit(time_ytocean_train, time_ytocean_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(time_ytocean_test, time_ytocean_test))\n",
    "\n",
    "encoder = Model(x,h)\n",
    "predict  = encoder.predict(time_ytocean_test)\n",
    "print(predict.shape)\n",
    "print(time_ytocean_test.shape)\n",
    "print(sys.getsizeof(time_ytocean_test))\n",
    "sys.getsizeof(predict)\n",
    "dataset.append('ocean_eta_t_2000_06.nc')\n",
    "bytes_before_compression.append(sys.getsizeof(time_ytocean_test))\n",
    "bytes_after_compression.append(sys.getsizeof(predict))\n",
    "compression_ratio.append(round((sys.getsizeof(time_ytocean_test)/sys.getsizeof(predict)),4))"
   ]
  }
 ]
}